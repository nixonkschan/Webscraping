{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382532ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "# Check if string is a number by converting it into a float\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) \n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# click on element until is available to be clicked \n",
    "def click_element(driver, xpath):\n",
    "    return WebDriverWait(driver, 120).until(EC.element_to_be_clickable((By.XPATH, xpath))).click()\n",
    "\n",
    "# get the number of item in the sub-cat\n",
    "def get_no_of_item():\n",
    "    # get total no of item in this category\n",
    "    ele_no_of_item = driver.find_element(By.XPATH,'//*[@id=\"headroomElement\"]/div/div/div/div/div[1]/div/span')\n",
    "    str_no_of_item = ele_no_of_item.text\n",
    "    print(f'str_no_of_item : {str_no_of_item}')\n",
    "    no_of_item = int(str_no_of_item.split()[0])\n",
    "    return no_of_item\n",
    "\n",
    "# click on loadmore button to show all items in the sub-cat\n",
    "def show_all_items(num_of_loadmore):\n",
    "    #Find and click on 'LOAD MORE' button to show all items\n",
    "        \n",
    "    # XPATH for loadmore button\n",
    "    if category == 'WOMEN' and sub_cat == 3:\n",
    "        xpath_loadmore ='//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/nav/div/div[2]/div[2]/div[1]/a/div/span[1]'\n",
    "    elif category == 'MEN' and sub_cat == 5:\n",
    "        xpath_loadmore ='//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/section[4]/div/div/div[2]/div[1]/a/div/span[1]'\n",
    "    else:\n",
    "        xpath_loadmore ='//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/section[3]/div/div/div[2]/div[1]/a'\n",
    "            \n",
    "    if num_of_loadmore > 0 :\n",
    "        time.sleep(2)\n",
    " \n",
    "        for x in range(num_of_loadmore):\n",
    "            time.sleep(1)\n",
    "            pyautogui.press('esc')\n",
    "            loadMoreButton = click_element(driver, xpath)\n",
    "            oadMoreButton.click()\n",
    "\n",
    "# Scrape all detail of the item such as review, category, sub-cat, name, price, description, \n",
    "# dictionary for the availability of color and size\n",
    "def get_item_details():\n",
    "    \n",
    "    item_dict = {}\n",
    "    \n",
    "    #Scrape the review score\n",
    "    list_of_em_tags=[]\n",
    "    em_tags = soup.find_all('em')\n",
    "\n",
    "    for em_tag in em_tags:\n",
    "        list_of_em_tags.append(em_tag.get_text())\n",
    "    if len(list_of_em_tags) >=2:\n",
    "        if is_number(list_of_em_tags[1].split()[0]):\n",
    "            item_dict['review'] = float(list_of_em_tags[1].split()[0])     \n",
    "    else:\n",
    "        item_dict['review']= 0\n",
    "\n",
    "    \n",
    "    #Scrape the product ID, sub-category, sub-sub-category, product name, description and price\n",
    "    item_dict['product_ID']= driver.find_element(By.XPATH,'//*[@id=\"left\"]/section/div[1]/h2/span[2]/em/span/span[2]').text\n",
    "    if category=='WOMEN':\n",
    "        item_dict['category'] ='WOMEN'\n",
    "    else:\n",
    "        item_dict['category'] ='MEN'\n",
    "    item_dict['sub_cat']= driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/div/div[1]/ol/li[3]/span').text\n",
    "    item_dict['ssub_cat']=driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/div/div[1]/ol/li[4]/a').text\n",
    "    item_dict['name'] = driver.find_element(By.XPATH, '//*[@id=\"right\"]/div[1]/div/div[1]/h1/span').text\n",
    "    item_dict['price'] = driver.find_element(By.XPATH, '//*[@id=\"right\"]/div[1]/div/div[2]/div[1]/div/div[1]/div/span/span/span').text\n",
    "    item_dict['desc'] = driver.find_element(By.XPATH, '//*[@id=\"right\"]/div[1]/div/div[3]/div').text\n",
    "        \n",
    "    color_sizes={}\n",
    "    size_avail={}\n",
    "    col_cnt=1\n",
    "\n",
    "    #Iterate all the colors until not found. Exception is expected\n",
    "    #Click on each color and capture the available sizes\n",
    "    while True:\n",
    "        try:\n",
    "            xpath = f'//*[@id=\"right\"]/div[2]/div[1]/div[1]/div/dl/div[{col_cnt}]/dt/label'\n",
    "            click_element(driver, xpath)\n",
    "            time.sleep(0.1)\n",
    "        except:\n",
    "            break;\n",
    "\n",
    "        # Scrap the color name\n",
    "        color_name = driver.find_element(By.XPATH,'//*[@id=\"right\"]/div[2]/div[1]/div[1]/div/h7/span').text\n",
    "\n",
    "        #load page source into BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source)\n",
    "\n",
    "        # Scrap all the size of the item for mapping use later\n",
    "        list_of_sizes=[]\n",
    "        for dl in soup.find_all('dl', class_ ='fr-chips _2jbyX3jUG0P1-2Zuj7ydQv'):\n",
    "            for el in dl:\n",
    "                if el.text in ['XXS', 'XS','S','M','L','XL','XXL',\\\n",
    "                       '22 inch', '23 inch','24 inch','25 inch','26 inch','27 inch','28 inch', \\\n",
    "                       '29 inch', '30 inch','31 inch','32 inch','33 inch',  \\\n",
    "                       '34 inch','35 inch','36 inch','37 inch','38 inch',  \\\n",
    "                       '24-26cm', '27-29cm', '27â€“29 cm', 'S/M', 'L/XL']:\n",
    "                    list_of_sizes.append(el.text)\n",
    "\n",
    "        input_element = soup.find_all('input', {'name': 'product-size-picker'})\n",
    "\n",
    "        i=0\n",
    "        size_avail={}\n",
    "        for size in input_element:\n",
    "            if size.has_attr('disabled'):\n",
    "                size_avail[list_of_sizes[i]] = 0\n",
    "            else:\n",
    "                size_avail[list_of_sizes[i]] = 1\n",
    "            i+=1\n",
    "\n",
    "        col_cnt +=1\n",
    "        color_sizes[color_name] = size_avail.copy()\n",
    "\n",
    "    item_dict['color_size_avail'] = color_sizes.copy()\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "all_item={}\n",
    "item_dict={}\n",
    "all_item_count=0\n",
    "\n",
    "categories =['WOMEN','MEN']\n",
    "\n",
    "no_of_itemperpage =24\n",
    "\n",
    "sub_cat_women =[1,2,3,4,5,6,7,8,9,12]\n",
    "sub_cat_men =[1,2,3,4,5,6,7,8,10,11,12]\n",
    "driver = webdriver.Chrome()\n",
    "    \n",
    "# Wait for the page to be loaded and send an\"ESC' Keypress to close the popup\n",
    "time.sleep(5)\n",
    "pyautogui.press('esc')\n",
    "time.sleep(2)\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    url = f'https://www.uniqlo.com/ca/en/{category}'\n",
    "    driver.get(url)\n",
    "    if category=='WOMEN':\n",
    "        sub_categoies=sub_cat_women\n",
    "    else:\n",
    "        sub_categoies=sub_cat_men\n",
    "    \n",
    "    for sub_cat in sub_categoies:\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            pyautogui.press('esc')\n",
    "            print(f'category : {category} , sub_cat :{sub_cat}')\n",
    "            \n",
    "            # Prepare the XPATH for sub-cat and click inside sub-categort afterward\n",
    "            xpath = f'//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/nav[2]/div/div[1]/article[{sub_cat}]/a/span'\n",
    "  \n",
    "            # Click inside the sub-cat\n",
    "            click_element(driver, xpath)\n",
    "            #WebDriverWait(driver, 120).until(EC.element_to_be_clickable((By.XPATH, xpath))).click()\n",
    "\n",
    "            time.sleep(5)\n",
    "            pyautogui.press('esc')\n",
    "    \n",
    "            #Get the no of item in the sub-cat\n",
    "            no_of_item = get_no_of_item()\n",
    "            \n",
    "            #Calculate the number of click required on 'LOAD MORE' button to show all items\n",
    "            num_of_loadmore = no_of_item//no_of_itemperpage -1\n",
    "            if no_of_item%no_of_itemperpage != 0:\n",
    "                num_of_loadmore +=1\n",
    "            \n",
    "            #Show all items in the sub-cat\n",
    "            show_all_items(num_of_loadmore)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(\"Exception #1: will skip the whole category\")\n",
    "            continue\n",
    "            print(e)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            for item_no in range(no_of_item):    \n",
    "                # Prepare the XPATH for each item\n",
    "                if category == 'MEN' and sub_cat==3:\n",
    "                    xpath=f'//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/section/div/div/div[1]/div[3]/div/div/article[{item_no+1}]/a/div/div[1]/div/img'\n",
    "                elif category == 'MEN' and sub_cat==5:\n",
    "                    xpath=f'//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/section[4]/div/div/div[1]/div[3]/div/div/article[{item_no+1}]/a/div/div[1]/div/img'\n",
    "                else:\n",
    "                    xpath=f'//*[@id=\"root\"]/div/div/div/div/main/div/div[2]/div/div/div/section[3]/div/div/div[1]/div[3]/div/div/article[{item_no+1}]/a'\n",
    "               \n",
    "                pyautogui.press('esc')\n",
    "                click_element(driver, xpath)\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # load the page souce into BeautifulSoup\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source)\n",
    "                pyautogui.press('esc')\n",
    "                \n",
    "                # Scrape the detail of the item\n",
    "                try :\n",
    "                    get_item_details()\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print('exception #3')\n",
    "                    print(e)\n",
    "                    continue\n",
    "                # Copy the item detail to item list\n",
    "                all_item[all_item_count+1]=item_dict.copy()\n",
    "                print(f'item_dict : {item_dict} \\n  all_item_count : {all_item_count} \\n')\n",
    "                all_item_count +=1\n",
    "                driver.back()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Exception #2\")\n",
    "            print(e)\n",
    "            \n",
    "        driver.back()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
